{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/07 09:28:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Lab 2\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Alice\", 34, \"F\"),\n",
    "    (\"Bob\", 45, \"M\"),\n",
    "    (\"Catherine\", 29, \"F\"),\n",
    "    (\"David\", 55, \"M\"),\n",
    "    (\"Eva\", 31, \"F\"),\n",
    "    (\"Yash\", 21, \"M\"),\n",
    "    (\"Dipanshu\", 20, \"M\"),\n",
    "    (\"Rhea\", 20, \"F\"),\n",
    "    (\"Anku\", 27, \"M\"),\n",
    "    (\"Anita\", 56, \"F\")\n",
    "]\n",
    "columns = [\"Name\", \"Age\", \"Gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, schema=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     Name|Age|Gender|\n",
      "+---------+---+------+\n",
      "|    Alice| 34|     F|\n",
      "|Catherine| 29|     F|\n",
      "|      Eva| 31|     F|\n",
      "|     Rhea| 20|     F|\n",
      "|    Anita| 56|     F|\n",
      "+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter(col(\"Gender\") == \"F\")\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+-----+\n",
      "|     Name|Age|Gender|Adult|\n",
      "+---------+---+------+-----+\n",
      "|    Alice| 34|     F|  Yes|\n",
      "|      Bob| 45|     M|  Yes|\n",
      "|Catherine| 29|     F|  Yes|\n",
      "|    David| 55|     M|  Yes|\n",
      "|      Eva| 31|     F|  Yes|\n",
      "|     Yash| 21|     M|  Yes|\n",
      "| Dipanshu| 20|     M|  Yes|\n",
      "|     Rhea| 20|     F|  Yes|\n",
      "|     Anku| 27|     M|  Yes|\n",
      "|    Anita| 56|     F|  Yes|\n",
      "+---------+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.withColumn(\"Adult\",lit(\"Yes\"))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     Name|Age|Gender|\n",
      "+---------+---+------+\n",
      "|    Alice| 34|     F|\n",
      "|Catherine| 29|     F|\n",
      "|      Eva| 31|     F|\n",
      "|     Rhea| 20|     F|\n",
      "|    Anita| 56|     F|\n",
      "+---------+---+------+\n",
      "\n",
      "Total Rows in filtered Dataframe = 5\n"
     ]
    }
   ],
   "source": [
    "filtered_count = filtered_df.count()\n",
    "filtered_df.show()\n",
    "print(f\"Total Rows in filtered Dataframe = {filtered_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     Name|Age|Gender|\n",
      "+---------+---+------+\n",
      "|    Alice| 34|     F|\n",
      "|      Bob| 45|     M|\n",
      "|Catherine| 29|     F|\n",
      "|    David| 55|     M|\n",
      "|      Eva| 31|     F|\n",
      "|     Yash| 21|     M|\n",
      "| Dipanshu| 20|     M|\n",
      "|     Rhea| 20|     F|\n",
      "|     Anku| 27|     M|\n",
      "|    Anita| 56|     F|\n",
      "+---------+---+------+\n",
      "\n",
      "Average Age = 33.8 & total age = 338\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as total_sum, avg\n",
    "total_age = df.select(total_sum(\"Age\")).collect()[0][0]\n",
    "avg_age = df.select(avg(\"Age\")).collect()[0][0]\n",
    "df.show()\n",
    "print(f\"Average Age = {avg_age} & total age = {total_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"output/dataframe_output.csv\"\n",
    "df.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|       for|    2|\n",
      "|  powerful|    1|\n",
      "|     great|    1|\n",
      "|     other|    1|\n",
      "|        is|    2|\n",
      "|      data|    2|\n",
      "|     Hello|    3|\n",
      "|       the|    1|\n",
      "|      tool|    1|\n",
      "|      from|    2|\n",
      "|   PySpark|    2|\n",
      "|     world|    2|\n",
      "|     Spark|    1|\n",
      "|         a|    1|\n",
      "|          |    2|\n",
      "|      side|    1|\n",
      "|processing|    2|\n",
      "|       big|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split, col\n",
    "txt = spark.read.text('./sample_text.txt')\n",
    "words_txt = txt.select(\n",
    "    explode(\n",
    "        split(col(\"value\"), \"\\\\s+\")\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "word_counts_txt = words_txt.groupBy(\"word\").count()\n",
    "word_counts_txt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
